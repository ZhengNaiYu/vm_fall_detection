# æ¨¡å‹æ¶æ„è§„èŒƒå‘½å

## ğŸ“‹ æ¨¡å‹æ–‡ä»¶ç»“æ„

### src/models/ ç›®å½•

```
src/models/
â”œâ”€â”€ __init__.py              # æ¨¡å‹å¯¼å‡ºæ¥å£
â”œâ”€â”€ lstm.py                  # æ ‡å‡†LSTMæ¨¡å‹
â”œâ”€â”€ gru.py                   # æ ‡å‡†GRUæ¨¡å‹
â”œâ”€â”€ bilstm_attention.py      # BiLSTM + Attention æ¨¡å‹
â”œâ”€â”€ vae.py                   # VAEæ¨¡å‹
â”œâ”€â”€ physical_rules.py        # ç‰©ç†è§„åˆ™æ£€æµ‹
â””â”€â”€ deprecated/              # å·²å¼ƒç”¨çš„æ–‡ä»¶
    â”œâ”€â”€ fall_detection_lstm.py     (use lstm.py instead)
    â”œâ”€â”€ fall_detection_gru.py      (use gru.py instead)
    â””â”€â”€ improved_lstm.py           (use bilstm_attention.py instead)
```

---

## ğŸ—ï¸ æ¨¡å‹å‘½åè§„èŒƒ

| æ–‡ä»¶å | ç±»å | æè¿° | ç”¨é€” |
|--------|------|------|------|
| `lstm.py` | `LSTM` | æ ‡å‡†LSTM | åŸºç¡€æ—¶åºåˆ†ç±» |
| `gru.py` | `GRU` | æ ‡å‡†GRU | è½»é‡çº§æ—¶åºåˆ†ç±» |
| `bilstm_attention.py` | `BiLSTMAttention` | åŒå‘LSTM+æ³¨æ„åŠ› | é«˜ç²¾åº¦åˆ†ç±» |
| `bilstm_attention.py` | `Transformer` | Transformerç¼–ç å™¨ | é•¿è·ç¦»ä¾èµ–æ•è· |

---

## ğŸ“ ä½¿ç”¨æ–¹å¼

### é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šæ¨¡å‹

```yaml
train_pose_detection:
  type: "BiLSTMAttention"     # é€‰é¡¹ï¼šLSTM, GRU, BiLSTMAttention, Transformer
  hidden_size: 128
  num_layers: 2
  dropout_prob: 0.4
```

### Pythonä»£ç ä¸­å¯¼å…¥

```python
from src.models import LSTM, GRU, BiLSTMAttention, Transformer

# åˆ›å»ºæ¨¡å‹
model = BiLSTMAttention(
    input_size=68,
    hidden_size=128,
    num_layers=2,
    num_classes=9,
    dropout_prob=0.4
)
```

---

## ğŸ”„ å‘åå…¼å®¹æ€§

ä¸ºäº†å…¼å®¹æ—§çš„ä»£ç ï¼Œæ¨¡å—æä¾›äº†åˆ«åæ˜ å°„ï¼š

```python
# è¿™äº›æ—§çš„å¯¼å…¥ä»ç„¶å¯ç”¨ï¼ˆå·²å¼ƒç”¨ï¼‰
from src.models import FallDetectionLSTM, FallDetectionGRU, ImprovedLSTM, TransformerEncoder

# å®ƒä»¬ä¼šè‡ªåŠ¨æ˜ å°„åˆ°æ–°çš„å‘½åï¼š
FallDetectionLSTM = LSTM
FallDetectionGRU = GRU
ImprovedLSTM = BiLSTMAttention
TransformerEncoder = Transformer
```

### æ—§é…ç½®å…¼å®¹

é…ç½®æ–‡ä»¶ä¸­ä»ç„¶æ”¯æŒæ—§çš„æ¨¡å‹åç§°ï¼ˆä¼šè‡ªåŠ¨è½¬æ¢ï¼‰ï¼š

```yaml
type: "ImprovedLSTM"        # âœ… ä»ç„¶æœ‰æ•ˆï¼ˆä¼šä½¿ç”¨BiLSTMAttentionï¼‰
type: "BiLSTMAttention"     # âœ… æ–°çš„è§„èŒƒåç§°
```

---

## ğŸ“Š æ¨¡å‹å¯¹æ¯”

| ç‰¹æ€§ | LSTM | GRU | BiLSTMAttention | Transformer |
|------|------|-----|-----------------|-------------|
| å‚æ•°é‡ | â­ | â­ | â­â­â­ | â­â­â­ |
| æ¨ç†é€Ÿåº¦ | â­â­â­ | â­â­â­ | â­â­ | â­ |
| ç²¾åº¦ | â­â­ | â­â­ | â­â­â­ | â­â­â­â­ |
| é•¿è·ç¦»ä¾èµ– | â­â­ | â­â­ | â­â­â­ | â­â­â­â­ |
| é€‚åˆåœºæ™¯ | ç®€å•åŠ¨ä½œ | è½»é‡çº§ | **æ¨è** | å¤æ‚åŠ¨ä½œ |

---

## ğŸš€ æ¨èé…ç½®

### ä¸€èˆ¬åœºæ™¯ï¼ˆæ¨èï¼‰
```yaml
train_pose_detection:
  type: "BiLSTMAttention"
  hidden_size: 128
  num_layers: 2
  dropout_prob: 0.4
```

### æ•°æ®é‡å°æˆ–æ¨ç†é€Ÿåº¦ä¼˜å…ˆ
```yaml
train_pose_detection:
  type: "LSTM"
  hidden_size: 64
  num_layers: 2
  dropout_prob: 0.5
```

### é«˜ç²¾åº¦éœ€æ±‚å’Œè®¡ç®—èƒ½åŠ›å……è¶³
```yaml
train_pose_detection:
  type: "Transformer"
  hidden_size: 256
  num_layers: 4
  nhead: 8
  dropout_prob: 0.3
```

---

## ğŸ“– ç±»è¯¦è§£

### LSTM
æ ‡å‡†LSTMå®ç°ï¼Œé€‚åˆåŸºç¡€æ—¶åºåˆ†ç±»ä»»åŠ¡ã€‚
- è¾“å…¥: (batch, seq_len, input_size)
- è¾“å‡º: (batch, num_classes)

### GRU
GRUæ˜¯LSTMçš„è½»é‡çº§æ›¿ä»£å“ï¼Œå‚æ•°æ›´å°‘ä½†è¡¨ç°ç›¸è¿‘ã€‚
- å¯¹å°æ•°æ®é›†æ›´å‹å¥½
- æ¨ç†é€Ÿåº¦æ›´å¿«

### BiLSTMAttention
- åŒå‘LSTMï¼šæ•è·å‰åæ–‡ä¿¡æ¯
- æ³¨æ„åŠ›æœºåˆ¶ï¼šå­¦ä¹ æ¯å¸§çš„é‡è¦æ€§æƒé‡
- æ·±åˆ†ç±»å™¨ï¼šå¢å¼ºç‰¹å¾æå–èƒ½åŠ›
- **æ¨èç”¨äºåŠ¨ä½œè¯†åˆ«**

### Transformer
- å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼šå¹¶è¡Œè®¡ç®—å¤šä¸ªè¡¨ç¤ºå­ç©ºé—´
- ä½ç½®ç¼–ç ï¼šä¿ç•™æ—¶åºä¿¡æ¯
- é€‚åˆé•¿åºåˆ—å’Œå¤æ‚æ—¶åºä¾èµ–

---

## ğŸ”§ è®­ç»ƒå‘½ä»¤

```bash
# ä½¿ç”¨æ ‡å‡†LSTM
python train_pose_sequence.py

# ä½¿ç”¨BiLSTMAttentionï¼ˆæ¨èï¼‰
python train_pose_sequence.py
# é…ç½®æ–‡ä»¶å·²é»˜è®¤ä½¿ç”¨BiLSTMAttention

# ä½¿ç”¨Transformer
# ä¿®æ”¹config.yamlä¸­ type: "Transformer"
python train_pose_sequence.py
```

---

## âš ï¸ è¿ç§»æŒ‡å—

å¦‚æœä½ æœ‰ä½¿ç”¨æ—§å‘½åçš„ä»£ç ï¼š

**æ—§ä»£ç ï¼ˆä»ç„¶æœ‰æ•ˆï¼‰ï¼š**
```python
from src.models import FallDetectionLSTM, ImprovedLSTM

model = ImprovedLSTM(input_size=68, hidden_size=128, ...)
```

**æ–°ä»£ç ï¼ˆæ¨èï¼‰ï¼š**
```python
from src.models import BiLSTMAttention

model = BiLSTMAttention(input_size=68, hidden_size=128, ...)
```

ä¸¤è€…å®Œå…¨ç­‰ä»·ï¼Œä½†æ¨èä½¿ç”¨æ–°çš„è§„èŒƒå‘½åä»¥æé«˜ä»£ç å¯è¯»æ€§ã€‚
